{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyObjyBCsxhV0nw/AY4P5vwZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sarahmahmoud16/NLP-Task/blob/main/Assignment_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tbNY09TTz4tB"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.datasets import imdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGV0sIwu0dOr",
        "outputId": "7263cb27-3d7b-4191-e1d2-963041bd2562"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    text = text.split()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = [lemmatizer.lemmatize(word) for word in text]\n",
        "\n",
        "    return \" \".join(text)"
      ],
      "metadata": {
        "id": "m0Par8ia0xPj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "max_length = 200\n",
        "embedding_dim = 128\n",
        "batch_size = 64\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "yO-p-7w30he2"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = {value: key for key, value in word_index.items()}\n",
        "def decode_review(encoded_review):\n",
        "    return \" \".join([reverse_word_index.get(i, \"?\") for i in encoded_review])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvliIhBN0rGM",
        "outputId": "041ff35f-7268-4774-9174-27451fa75578"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)"
      ],
      "metadata": {
        "id": "Ky5CUVr80k7h"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_texts = [preprocess_text(decode_review(review)) for review in x_train]\n",
        "x_test_texts = [preprocess_text(decode_review(review)) for review in x_test]"
      ],
      "metadata": {
        "id": "2Ox6C4d51DP1"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")  # Ensure unseen words get mapped\n",
        "tokenizer.fit_on_texts(x_train_texts)\n",
        "x_train_seq = tokenizer.texts_to_sequences(x_train_texts)\n",
        "x_test_seq = tokenizer.texts_to_sequences(x_test_texts)"
      ],
      "metadata": {
        "id": "I-eG0-191OXR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_pad = pad_sequences(x_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
        "x_test_pad = pad_sequences(x_test_seq, maxlen=max_length, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "7tfnC5yq1TsQ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 128, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),  # Helps prevent overfitting\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n"
      ],
      "metadata": {
        "id": "ejY3O0Gc1YZc"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "6lieiXLL1c5e"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train_pad, y_train, epochs=epochs, batch_size=64, validation_data=(x_test_pad, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE9zG-9w1fyc",
        "outputId": "5eef4f07-c9f9-47a4-8fce-7982c925e6be"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.9357 - loss: 0.1944 - val_accuracy: 0.8463 - val_loss: 0.3860\n",
            "Epoch 2/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 57ms/step - accuracy: 0.9533 - loss: 0.1607 - val_accuracy: 0.8460 - val_loss: 0.3999\n",
            "Epoch 3/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 58ms/step - accuracy: 0.9633 - loss: 0.1343 - val_accuracy: 0.8373 - val_loss: 0.4730\n",
            "Epoch 4/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 67ms/step - accuracy: 0.9703 - loss: 0.1091 - val_accuracy: 0.8345 - val_loss: 0.5251\n",
            "Epoch 5/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - accuracy: 0.9770 - loss: 0.0973 - val_accuracy: 0.8328 - val_loss: 0.5839\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78439c6cfb90>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(x_test_pad, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y0_nwg_1mDN",
        "outputId": "5586d976-c4cf-4f57-937e-4297f3bd4e3b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.8348 - loss: 0.5760\n",
            "Test Accuracy: 0.8328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text):\n",
        "    # Preprocess the input text\n",
        "    cleaned_text = preprocess_text(text)\n",
        "\n",
        "    # Convert to sequence\n",
        "    seq = tokenizer.texts_to_sequences([cleaned_text])\n",
        "\n",
        "    # Pad the sequence\n",
        "    pad_seq = pad_sequences(seq, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(pad_seq)[0][0]\n",
        "\n",
        "    # Interpret result\n",
        "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative \"\n",
        "    print(f\"Review: {text}\\nSentiment: {sentiment} (Score: {prediction:.4f})\")\n",
        "\n",
        "# Test with some examples\n",
        "predict_sentiment(\"I love this movie! It's fantastic.\")\n",
        "predict_sentiment(\"This movie is bad! I hate it.\")\n",
        "predict_sentiment(\"The movie is very beautiful! I will watch it again.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAPJZL5_9FpV",
        "outputId": "8a6a5e09-cb60-4572-aac3-19d5828aa73a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Review: I love this movie! It's fantastic.\n",
            "Sentiment: Positive (Score: 0.6459)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Review: This movie is bad! I hate it.\n",
            "Sentiment: Negative  (Score: 0.2078)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Review: The movie is very beautiful! I will watch it again.\n",
            "Sentiment: Positive (Score: 0.5443)\n"
          ]
        }
      ]
    }
  ]
}